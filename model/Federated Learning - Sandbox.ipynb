{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/k8DCqH99hvltV+qsZbxN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **For Test Usage**\n","\n","This notebook is created for test Federated Learning process in different framework like Tensorflow federated and FLower."],"metadata":{"id":"Udx90HII0yNW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAsSYk5O3HGP"},"outputs":[],"source":["# pip install tensorflow tensorflow-federated"]},{"cell_type":"code","source":["# ÙŽimport tensorflow as tf\n","# import tensorflow_federated as tff"],"metadata":{"id":"GxPjyw4m3M9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n","# emnist_train = emnist_train.create_tf_dataset_from_all_clients()\n","# emnist_test = emnist_test.create_tf_dataset_from_all_clients()\n","\n","# def preprocess(dataset):\n","#     def element_fn(element):\n","#         return (tf.expand_dims(element['pixels'], axis=-1), element['label'])\n","\n","#     return dataset.map(element_fn)\n","\n","# train_data = preprocess(emnist_train).shuffle(500).batch(20)\n","# test_data = preprocess(emnist_test).batch(100)"],"metadata":{"id":"JDkVFsKc3PGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def create_model():\n","#     model = tf.keras.Sequential([\n","#         tf.keras.layers.Input(shape=(28, 28, 1)),\n","#         tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n","#         tf.keras.layers.MaxPooling2D((2, 2)),\n","#         tf.keras.layers.Flatten(),\n","#         tf.keras.layers.Dense(47, activation='softmax')  # 47 classes for EMNIST letters\n","#     ])\n","#     return model"],"metadata":{"id":"aEtPxYv73R7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def model_fn():\n","#     keras_model = create_model()\n","\n","#     return tff.learning.models.from_keras_model(\n","#         keras_model,\n","#         input_spec=train_data.element_spec,\n","#         loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n","#     )"],"metadata":{"id":"zoU2IG5E3pfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n","#     model_fn,\n","#     client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n","#     server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n","# )"],"metadata":{"id":"Hg6nZpF-3uAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NUM_ROUNDS = 10\n","# state = iterative_process.initialize()\n","\n","# for round_num in range(NUM_ROUNDS):\n","#     state, metrics = iterative_process.next(state, train_data)\n","#     print(f'Round {round_num}: {metrics}')"],"metadata":{"id":"mIHO4USC4YVF","executionInfo":{"status":"ok","timestamp":1693589850043,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mohammaderfan Ghasemi","userId":"03613741105621501267"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# result = training_process.next(train_state, train_data)\n","# train_state = result.state\n","# train_metrics = result.metrics\n","# print('round  1, metrics={}'.format(train_metrics))"],"metadata":{"id":"McqxPRb64qME","executionInfo":{"status":"ok","timestamp":1693589863359,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mohammaderfan Ghasemi","userId":"03613741105621501267"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# # Copyright 2020, The TensorFlow Federated Authors.\n","# #\n","# # Licensed under the Apache License, Version 2.0 (the \"License\");\n","# # you may not use this file except in compliance with the License.\n","# # You may obtain a copy of the License at\n","# #\n","# #      http://www.apache.org/licenses/LICENSE-2.0\n","# #\n","# # Unless required by applicable law or agreed to in writing, software\n","# # distributed under the License is distributed on an \"AS IS\" BASIS,\n","# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# # See the License for the specific language governing permissions and\n","# # limitations under the License.\n","# \"\"\"An implementation of the Federated Averaging algorithm.\n","\n","# This is intended to be a minimal stand-alone implementation of Federated\n","# Averaging, suitable for branching as a starting point for algorithm\n","# modifications; see `tff.learning.algorithms.build_weighted_fed_avg` for a more\n","# full-featured implementation.\n","\n","# Based on the paper:\n","\n","# Communication-Efficient Learning of Deep Networks from Decentralized Data\n","#     H. Brendan McMahan, Eider Moore, Daniel Ramage,\n","#     Seth Hampson, Blaise Aguera y Arcas. AISTATS 2017.\n","#     https://arxiv.org/abs/1602.05629\n","# \"\"\"\n","\n","# from typing import Any\n","\n","# import attrs\n","# import tensorflow as tf\n","# import tensorflow_federated as tff\n","\n","\n","# # TODO(b/295181362): Update from `Any` to a more specific type.\n","# @attrs.define(eq=False, frozen=True)\n","# class ClientOutput:\n","#   \"\"\"Structure for outputs returned from clients during federated optimization.\n","\n","#   Attributes:\n","#     weights_delta: A dictionary of updates to the model's trainable variables.\n","#     client_weight: Weight to be used in a weighted mean when aggregating\n","#       `weights_delta`.\n","#     model_output: A structure matching\n","#       `tff.learning.models.VariableModel.report_local_unfinalized_metrics`,\n","#       reflecting the results of training on the input dataset.\n","#   \"\"\"\n","\n","#   weights_delta: Any\n","#   client_weight: tf.float32\n","#   model_output: Any\n","\n","\n","# @attrs.define(eq=False, frozen=True)\n","# class ServerState:\n","#   \"\"\"Structure for state on the server.\n","\n","#   Attributes:\n","#     model:  A `tff.learning.models.ModelWeights` structure, containing Tensors\n","#       or Variables.\n","#     optimizer_state: Variables of optimizer.\n","#     round_num: The current round in the training process.\n","#   \"\"\"\n","\n","#   model: tff.learning.models.ModelWeights\n","#   optimizer_state: Any\n","#   round_num: int\n","\n","\n","# @attrs.define(eq=False, frozen=True)\n","# class BroadcastMessage:\n","#   \"\"\"Structure for tensors broadcasted by server during federated optimization.\n","\n","#   Attributes:\n","#     model_weights: A `tff.learning.models.ModelWeights` structure, containing\n","#       Tensors or Variables.\n","#     round_num: Round index to broadcast. We use `round_num` as an example to\n","#       show how to broadcast auxiliary information that can be helpful on\n","#       clients. It is not explicitly used, but can be applied to enable learning\n","#       rate scheduling.\n","#   \"\"\"\n","\n","#   model_weights: tff.learning.models.ModelWeights\n","#   round_num: int\n","\n","\n","# @tf.function\n","# def server_update(model, server_optimizer, server_state, weights_delta):\n","#   \"\"\"Updates `server_state` based on `weights_delta`.\n","\n","#   Args:\n","#     model: A `KerasModelWrapper` or `tff.learning.models.VariableModel`.\n","#     server_optimizer: A `tf.keras.optimizers.Optimizer`. If the optimizer\n","#       creates variables, they must have already been created.\n","#     server_state: A `ServerState`, the state to be updated.\n","#     weights_delta: A nested structure of tensors holding the updates to the\n","#       trainable variables of the model.\n","\n","#   Returns:\n","#     An updated `ServerState`.\n","#   \"\"\"\n","#   # Initialize the model with the current state.\n","#   model_weights = tff.learning.models.ModelWeights.from_model(model)\n","#   tf.nest.map_structure(\n","#       lambda v, t: v.assign(t), model_weights, server_state.model\n","#   )\n","#   tf.nest.map_structure(\n","#       lambda v, t: v.assign(t),\n","#       server_optimizer.variables(),\n","#       server_state.optimizer_state,\n","#   )\n","\n","#   # Apply the update to the model.\n","#   neg_weights_delta = [-1.0 * x for x in weights_delta]\n","#   server_optimizer.apply_gradients(\n","#       zip(neg_weights_delta, model_weights.trainable), name='server_update'\n","#   )\n","\n","#   # Create a new state based on the updated model.\n","#   return tff.structure.update_struct(\n","#       server_state,\n","#       model=model_weights,\n","#       optimizer_state=server_optimizer.variables(),\n","#       round_num=server_state.round_num + 1,\n","#   )\n","\n","\n","# @tf.function\n","# def build_server_broadcast_message(server_state):\n","#   \"\"\"Builds `BroadcastMessage` for broadcasting.\n","\n","#   This method can be used to post-process `ServerState` before broadcasting.\n","#   For example, perform model compression on `ServerState` to obtain a compressed\n","#   state that is sent in a `BroadcastMessage`.\n","\n","#   Args:\n","#     server_state: A `ServerState`.\n","\n","#   Returns:\n","#     A `BroadcastMessage`.\n","#   \"\"\"\n","#   return BroadcastMessage(\n","#       model_weights=server_state.model, round_num=server_state.round_num\n","#   )\n","\n","\n","# @tf.function\n","# def batch_client_update(\n","#     model, batch, initial_weights, num_examples, client_optimizer\n","# ):\n","#   \"\"\"Performs client local training of `model` on `dataset`.\n","\n","#   Args:\n","#     model: A `tff.learning.models.VariableModel` to train locally on the client.\n","#     batch: A batch from 'tf.data.Dataset' representing the clients local data.\n","#     initial_weights: initial model weights to use for update. weights to train.\n","#     num_examples: Number of examples observed so far.\n","#     client_optimizer: A `tf.keras.optimizers.Optimizer` used to update the local\n","#       model during training.\n","\n","#   Returns:\n","#     A `ClientOutput` instance with a model update to aggregate on the server.\n","#   \"\"\"\n","#   model_weights = tff.learning.models.ModelWeights.from_model(model)\n","#   tf.nest.map_structure(\n","#       lambda v, t: v.assign(t), model_weights.trainable, initial_weights\n","#   )\n","\n","#   num_examples = tf.cast(num_examples, tf.int32)\n","#   with tf.GradientTape() as tape:\n","#     outputs = model.forward_pass(batch)\n","#   grads = tape.gradient(outputs.loss, model_weights.trainable)\n","#   client_optimizer.apply_gradients(zip(grads, model_weights.trainable))\n","#   batch_size = tf.shape(batch['y'])[0]\n","#   num_examples += batch_size\n","\n","#   weights_delta = tf.nest.map_structure(\n","#       lambda a, b: a - b, model_weights.trainable, initial_weights\n","#   )\n","#   client_weight = tf.cast(num_examples, tf.float32)\n","#   model_outputs = model.report_local_unfinalized_metrics()\n","#   return ClientOutput(weights_delta, client_weight, model_outputs)\n","\n","\n","# @tf.function\n","# def init_client_ouput(model, server_message):\n","#   client_weight = tf.constant(0, dtype=tf.float32)\n","#   return ClientOutput(\n","#       server_message.model_weights.trainable,\n","#       client_weight,\n","#       model.report_local_unfinalized_metrics(),\n","#   )\n","\n","\n","# @tf.function\n","# def client_update(model, dataset, server_message, client_optimizer):\n","#   \"\"\"Performans client local training of `model` on `dataset`.\n","\n","#   Args:\n","#     model: A `tff.learning.models.VariableModel` to train locally on the client.\n","#     dataset: A 'tf.data.Dataset' representing the clients local dataset.\n","#     server_message: A `BroadcastMessage` from serve containing the initial model\n","#       weights to train.\n","#     client_optimizer: A `tf.keras.optimizers.Optimizer` used to update the local\n","#       model during training.\n","\n","#   Returns:\n","#     A `ClientOutput` instance with a model update to aggregate on the server.\n","#   \"\"\"\n","#   model_weights = tff.learning.models.ModelWeights.from_model(model)\n","#   initial_weights = server_message.model_weights\n","#   tf.nest.map_structure(\n","#       lambda v, t: v.assign(t), model_weights, initial_weights\n","#   )\n","\n","#   num_examples = tf.constant(0, dtype=tf.int32)\n","#   # Explicit use `iter` for dataset is a trick that makes TFF more robust in\n","#   # GPU simulation and slightly more performant in the unconventional usage\n","#   # of large number of small datasets.\n","#   for batch in iter(dataset):\n","#     with tf.GradientTape() as tape:\n","#       outputs = model.forward_pass(batch)\n","#     grads = tape.gradient(outputs.loss, model_weights.trainable)\n","#     client_optimizer.apply_gradients(zip(grads, model_weights.trainable))\n","#     batch_size = tf.shape(batch['y'])[0]\n","#     num_examples += batch_size\n","\n","#   weights_delta = tf.nest.map_structure(\n","#       lambda a, b: a - b, model_weights.trainable, initial_weights.trainable\n","#   )\n","#   client_weight = tf.cast(num_examples, tf.float32)\n","#   model_outputs = model.report_local_unfinalized_metrics()\n","#   return ClientOutput(weights_delta, client_weight, model_outputs)"],"metadata":{"id":"wPO5nKXS-oph"},"execution_count":null,"outputs":[]}]}